{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Image Style Transfer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember using applications like Prisma, Snapseed, Lucid etc.?\n",
    " Ever wondered how these things works? \n",
    "\n",
    "We give a photo from our camera roll, then we select a design to combine both the images and we get a resultant new image which has the **_content_** of our input image and **_style_** of the design image. In the world of deep learning this is called **“Style transfer”**\n",
    "\n",
    "The **_style_** of a painting is: the way the painter used brush strokes; how these strokes form objects; texture of objects; color palette used.\n",
    "\n",
    "The **_content_** of the image is what objects are present in this image (person, face, dog, eyes, etc.) and their relationships in space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some examples:\n",
    "<img src=\"Collage.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What papers are the study based on?\n",
    "- Leon A. Gatys : A Neural Algorithm of Artistic Style\n",
    "- Justin Johnson : Perceptual Losses for Real-Time Style Transfer and Super-Resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the requirements?\n",
    "- Pre-trained VGG19 network \n",
    "- TensorFlow (v 1.9.0)\n",
    "- Numpy & Scipy\n",
    "- Pillow or PIL (Python Imaging Library) – library for image manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we separate style and content of an image?\n",
    "#### By using convolutional neural networks(CNNs)\n",
    "_Wiki defn. a convolutional neural network (CNN) is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They were inspired by biological process, namely the connectivity pattern between neurons resembles the organization of the animal visual cortex._\n",
    "\n",
    "- filters in the first layers in CNNs recognize simple patterns, brush strokes, textures, etc.\n",
    "- filters in the intermediate layers happen to locate and recognize major objects in the image, such as a dog, a building or a mountain.\n",
    "\n",
    "The reason why a pre-trained network is being used is that, when we take a convolutional neural network that has already been trained to recognize objects within images then that network will have developed some internal independent representations of the content and style contained within a given image. Here in a VGG net, shallow layers learns low level features and as we go deeper into the network these convolutional layers are able to represent much larger scale features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from PIL import Image\n",
    "from sys import stderr\n",
    "import scipy.io\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import imageio\n",
    "from skimage import transform\n",
    "import skimage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize known hyperparameters and other parameters necessary for the style transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content and style ratio 5e0:5e2\n",
    "# Amout of content image in the result image reconstruction. Default = 5e0.\n",
    "CONTENT_WEIGHT = 5e0\n",
    "\n",
    "# Content and style ratio. Amout of style image in the result image reconstruction. Default = 1e2.\n",
    "STYLE_WEIGHT = 5e2 \n",
    "\n",
    "# Weight of total-variation (TV) regularization; this helps to smooth the image. Default is 1e-3.\n",
    "TV_WEIGHT = 1e2 \n",
    "\n",
    "# Used to tweak how \"abstract\" the style transfer should be. Lower values mean that \n",
    "# style transfer of a finer features will be favored over style transfer of a more coarse features,and vice versa.\n",
    "# Default value is 1\n",
    "STYLE_LAYER_WEIGHT_EXP = 1 \n",
    "                           \n",
    "# Specifies the coefficient of content transfer layers. Default value = 1. \n",
    "CONTENT_WEIGHT_BLEND = 1 \n",
    "\n",
    "STYLE_BLEND_WEIGHTS = (0.2,0.2) #''''The weight for blending the style of multiple style images, \n",
    "                                #as a comma-separated list, such as -style_blend_weights 3,7. \n",
    "                                #By default all style images are equally weighted.''''\n",
    "# Doubling style has better effect than halving content\n",
    "# Halving the content weight will result in lower absolute values for the loss function. \n",
    "\n",
    "\n",
    "#Optimize Parameters\n",
    "LEARNING_RATE = 1e1 # Learning rate to use with ADAM optimizer. Default is 1e1. \n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-08\n",
    "STYLE_SCALE = 1 # Scale at which to extract features from the style image. Default is 1.0.\n",
    "ITERATIONS = 5 # Controls no of iterations, more the iteration, better the resultant image\n",
    "VGG_PATH = './imagenet-vgg-verydeep-19.mat'\n",
    "STYLE_PATH = './rain-princess-aframov.jpg' # The styling image \n",
    "CONTENT_PATH = './content2.jpg' # The content image \n",
    "OUTPUT = './result.jpg' # Result after style transfer \n",
    "POOLING = 'max' #''''Allows to select which pooling layers to use (specify either max or avg). \n",
    "                 #   The outputs are perceptually differnt, max pool in \n",
    "                  #  general tends to have finer detail style transfer, but could have troubles at lower-freqency detail level''''\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "#Other Setting\n",
    "PRESERVE_COLORS = False\n",
    "\n",
    "logs_path = 'C:/Users/andre/Documents/Jupyter Notebooks Summer Semester/tensor_logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The VGG19 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the VGG19 latyers\n",
    "#''''vgg19 returns a pretrained VGG-19 model. This model is trained on a subset of the ImageNet database [1], \n",
    " #   which is used in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) [2]. \n",
    "  #  VGG-19 is trained on more than a million images and can classify images into 1000 object categories. \n",
    "   # For example, keyboard, mouse, pencil, and many animals. As a result, the model has learned rich \n",
    "    #feature representations for a wide range of images.''''\n",
    "\n",
    "VGG19_LAYERS = (\n",
    "    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', \n",
    "    'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', \n",
    "    'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', \n",
    "    'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of required Relu layers for Content and Style layers from the VGG19 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default layers as described in the paper\n",
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS   = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(VGG_PATH):\n",
    "        print(\"Network %s does not exist. (VGG19 File not found)\" % VGG_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained VGG19 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(data_path):\n",
    "    data = scipy.io.loadmat(data_path)\n",
    "    if not all(i in data for i in ('layers', 'classes', 'normalization')):\n",
    "        raise ValueError(\"Wrong VGG19 data. Please download the correct data.\")\n",
    "    mean = data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(mean, axis=(0, 1))\n",
    "    weights = data['layers'][0]\n",
    "    return weights, mean_pixel\n",
    "\n",
    "def preprocess(image, mean_pixel):\n",
    "    return image - mean_pixel\n",
    "\n",
    "def unprocess(image, mean_pixel):\n",
    "    return image + mean_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to define the network layers according to the layer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In progress - The code yet to be fully Modified. \n",
    "def net_preloaded(weights, input_image, pooling):\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(VGG19_LAYERS):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            current = _conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = _pool_layer(current, pooling)\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(VGG19_LAYERS)\n",
    "    return net\n",
    "\n",
    "# Convolution layer values\n",
    "def _conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
    "            padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "# Pool layer values\n",
    "def _pool_layer(input, pooling):\n",
    "    if pooling == 'avg':\n",
    "        return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to perform style transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Progress - fine tuning required\n",
    "\n",
    "# this function yields tuples (iteration, image);\n",
    "def stylize(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations,\n",
    "        content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight,\n",
    "        learning_rate, beta1, beta2, epsilon, pooling,\n",
    "        print_iterations=None, checkpoint_iterations=None):\n",
    "    \n",
    "    shape = (1,) + content.shape\n",
    "    style_shapes = [(1,) + style.shape for style in styles]\n",
    "    content_features = {}\n",
    "    style_features = [{} for _ in styles]\n",
    "\n",
    "    vgg_weights, vgg_mean_pixel = load_net(network)\n",
    "\n",
    "    layer_weight = 1.0\n",
    "    style_layers_weights = {}\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] = layer_weight\n",
    "        layer_weight *= style_layer_weight_exp\n",
    "\n",
    "    # normalize style layer weights\n",
    "    layer_weights_sum = 0\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        layer_weights_sum += style_layers_weights[style_layer]\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] /= layer_weights_sum\n",
    "\n",
    "    # to compute content features in feedforward mode\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=shape)\n",
    "        net = net_preloaded(vgg_weights, image, pooling)\n",
    "        content_pre = np.array([preprocess(content, vgg_mean_pixel)])\n",
    "        for layer in CONTENT_LAYERS:\n",
    "            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})\n",
    "            \n",
    "    # to compute style features in feedforward mode\n",
    "    for i in range(len(styles)):\n",
    "        g = tf.Graph()\n",
    "        with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "            image = tf.placeholder('float', shape=style_shapes[i])\n",
    "            net = net_preloaded(vgg_weights, image, pooling)\n",
    "            style_pre = np.array([preprocess(styles[i], vgg_mean_pixel)])\n",
    "            for layer in STYLE_LAYERS:\n",
    "                features = net[layer].eval(feed_dict={image: style_pre})\n",
    "                features = np.reshape(features, (-1, features.shape[3]))\n",
    "                gram = np.matmul(features.T, features) / features.size\n",
    "                style_features[i][layer] = gram\n",
    "                \n",
    "                \n",
    "    initial_content_noise_coeff = 1.0 - initial_noiseblend\n",
    "\n",
    "    # to make stylized image using backpropogation\n",
    "    with tf.Graph().as_default():\n",
    "        if initial is None:\n",
    "            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "            initial = tf.random_normal(shape) * 0.256\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            initial = np.array([preprocess(initial, vgg_mean_pixel)])\n",
    "            initial = initial.astype('float32')\n",
    "            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "            initial = (initial) * initial_content_noise_coeff + (tf.random_normal(shape) * 0.256) * (1.0 - initial_content_noise_coeff)\n",
    "            \n",
    "            \n",
    "            \n",
    "        image = tf.Variable(initial)\n",
    "        net = net_preloaded(vgg_weights, image, pooling)\n",
    "\n",
    "        # to compute the content loss\n",
    "        content_layers_weights = {}\n",
    "        content_layers_weights['relu4_2'] = content_weight_blend\n",
    "        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend\n",
    "\n",
    "        content_loss = 0\n",
    "        content_losses = []\n",
    "        for content_layer in CONTENT_LAYERS:\n",
    "            content_losses.append(content_layers_weights[content_layer] * content_weight * (2 * tf.nn.l2_loss(\n",
    "                    net[content_layer] - content_features[content_layer]) /\n",
    "                    content_features[content_layer].size))   \n",
    "        content_loss += reduce(tf.add, content_losses)\n",
    "        contentlosssumm = tf.summary.scalar(\"Content_Loss\", content_loss)\n",
    "            \n",
    "\n",
    "        # to compute the style loss\n",
    "        style_loss = 0\n",
    "        for i in range(len(styles)):\n",
    "            style_losses = []\n",
    "            for style_layer in STYLE_LAYERS:\n",
    "                layer = net[style_layer]\n",
    "                _, height, width, number = map(lambda i: i.value, layer.get_shape())#\n",
    "                size = height * width * number\n",
    "                feats = tf.reshape(layer, (-1, number))\n",
    "                gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "                style_gram = style_features[i][style_layer]\n",
    "                style_losses.append(style_layers_weights[style_layer] * 2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "            stylelosssumm = tf.summary.scalar(\"Style_Loss\", style_loss)\n",
    "                \n",
    "        \n",
    "        # to compute the overall loss\n",
    "        loss = content_loss + style_loss #+ tv_loss    \n",
    "        \n",
    "        # optimizer setup, add it to the graph and return an tf.Operation\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "                       \n",
    "        # to print loss figures after the final iteration\n",
    "        def print_progress():\n",
    "            stderr.write('  Content Loss: %g\\n' % content_loss.eval())\n",
    "            stderr.write('    Style Loss: %g\\n' % style_loss.eval())\n",
    "            stderr.write('    TOTAL Loss: %g\\n' % loss.eval())\n",
    "\n",
    "        # Optimization\n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            stderr.write('Optimization started...\\n')\n",
    "            if (print_iterations and print_iterations != 0):\n",
    "                print_progress()\n",
    "            for i in range(iterations):\n",
    "                stderr.write('Iteration %4d/%4d\\n' % (i + 1, iterations))\n",
    "                train_step.run()\n",
    "\n",
    "                last_step = (i == iterations - 1)\n",
    "                if last_step or (print_iterations and i % print_iterations == 0):\n",
    "                    print_progress()\n",
    "\n",
    "                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "                    this_loss = loss.eval()\n",
    "                    if this_loss < best_loss:\n",
    "                        best_loss = this_loss\n",
    "                        best = image.eval()\n",
    "\n",
    "                    img_out = unprocess(best.reshape(shape[1:]), vgg_mean_pixel)\n",
    "\n",
    "                    if preserve_colors and preserve_colors == True:\n",
    "                        original_image = np.clip(content, 0, 255)\n",
    "                        styled_image = np.clip(img_out, 0, 255)\n",
    "\n",
    "                        # Luminosity transfer steps:\n",
    "                        # 1. Convert stylized RGB->grayscale accoriding to Rec.601 luma (0.299, 0.587, 0.114)\n",
    "                        # 2. Convert stylized grayscale into YUV (YCbCr)\n",
    "                        # 3. Convert original image into YUV (YCbCr)\n",
    "                        # 4. Recombine (stylizedYUV.Y, originalYUV.U, originalYUV.V)\n",
    "                        # 5. Convert recombined image from YUV back to RGB\n",
    "\n",
    "                        # 1\n",
    "                        styled_grayscale = rgb2gray(styled_image)\n",
    "                        styled_grayscale_rgb = gray2rgb(styled_grayscale)\n",
    "\n",
    "                        # 2\n",
    "                        styled_grayscale_yuv = np.array(Image.fromarray(styled_grayscale_rgb.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                        # 3\n",
    "                        original_yuv = np.array(Image.fromarray(original_image.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                        # 4\n",
    "                        w, h, _ = original_image.shape\n",
    "                        combined_yuv = np.empty((w, h, 3), dtype=np.uint8)\n",
    "                        combined_yuv[..., 0] = styled_grayscale_yuv[..., 0]\n",
    "                        combined_yuv[..., 1] = original_yuv[..., 1]\n",
    "                        combined_yuv[..., 2] = original_yuv[..., 2]\n",
    "\n",
    "                        # 5\n",
    "                        img_out = np.array(Image.fromarray(combined_yuv, 'YCbCr').convert('RGB'))\n",
    "\n",
    "\n",
    "                    yield (\n",
    "                        (None if last_step else i),\n",
    "                        img_out\n",
    "                    )\n",
    "                                         \n",
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to preserve the color of content image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def gray2rgb(gray):\n",
    "    w, h = gray.shape\n",
    "    rgb = np.empty((w, h, 3), dtype=np.float32)\n",
    "    rgb[:, :, 2] = rgb[:, :, 1] = rgb[:, :, 0] = gray\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to load source image from disk\n",
    "def imread(path):\n",
    "    #read an image from a file as an array\n",
    "    img = scipy.misc.imread(path).astype(np.float)\n",
    "    if len(img.shape) == 2:\n",
    "        # grayscale\n",
    "        img = np.dstack((img,img,img))\n",
    "    elif img.shape[2] == 4:\n",
    "        # JPG with alpha channel\n",
    "        img = img[:,:,:3]\n",
    "    return img\n",
    "\n",
    "# Method to save stylized image\n",
    "def imsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img).save(path, quality=95)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "   # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = imageio.imread(CONTENT_PATH)\n",
    "style_images = [imageio.imread(STYLE_PATH)]\n",
    "target_shape = content_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting size and shape of style image to fit the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_scale = STYLE_SCALE\n",
    "for i in range(len(style_images)):\n",
    "    #print(style_images[i].shape[1])\n",
    "    #style_scale * target_shape[1] / style_images[i].shape[1] - use this as input\n",
    "    #print(\"the output shape is {}\".format(style_scale * target_shape[1]/ style_images[i].shape[1]))\n",
    "    style_images[i] = skimage.transform.resize(image=style_images[i], output_shape=target_shape,mode='constant')\n",
    "\n",
    "#In default, for every style image, the style weight is the same. Possibility to set weights of different style images\n",
    "style_blend_weights = [1.0/len(style_images) for _ in style_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In default, for every style image, the style weight is the same. \n",
    "initial_noiseblend = 1.0\n",
    "initial = content_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to begin iteration based on the stylize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization started...\n",
      "Iteration    1/   5\n",
      "Iteration    2/   5\n",
      "Iteration    3/   5\n",
      "Iteration    4/   5\n",
      "Iteration    5/   5\n",
      "  Content Loss: 605030\n",
      "    Style Loss: 3.3767e+06\n",
      "    TOTAL Loss: 3.98173e+06\n"
     ]
    }
   ],
   "source": [
    "# Iterations and optimization and calculating the content loss, style loss, total loss. \n",
    "#merge = tf.summary.merge([contentlosssumm, stylelosssumm, totallosssum])\n",
    "for iteration, image in stylize(\n",
    "        network                =VGG_PATH,\n",
    "        initial                =initial,\n",
    "        initial_noiseblend     =initial_noiseblend,\n",
    "        content                =content_image,\n",
    "        styles                 =style_images,\n",
    "        preserve_colors        =PRESERVE_COLORS,\n",
    "        iterations             =ITERATIONS,\n",
    "        content_weight         =CONTENT_WEIGHT,\n",
    "        content_weight_blend   =CONTENT_WEIGHT_BLEND,\n",
    "        style_weight           =STYLE_WEIGHT,\n",
    "        style_layer_weight_exp =STYLE_LAYER_WEIGHT_EXP,\n",
    "        style_blend_weights    =STYLE_BLEND_WEIGHTS,\n",
    "        tv_weight              =TV_WEIGHT,\n",
    "        learning_rate          =LEARNING_RATE,\n",
    "        beta1=BETA1,\n",
    "        beta2=BETA2,\n",
    "        epsilon=EPSILON,\n",
    "        pooling                =POOLING,\n",
    "        print_iterations       =None,\n",
    "        checkpoint_iterations  =None\n",
    "    ):\n",
    "       \n",
    "        \n",
    "        output_file = None\n",
    "        combined_rgb = image\n",
    "        output_file = OUTPUT\n",
    "        \n",
    "        if output_file:\n",
    "            imsave(output_file, combined_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result of the above run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"result.jpg\" alt=\"Drawing\" style=\"width: 500px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further results:\n",
    "<img src=\"result_collage.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from pre-trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conv 4.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conv 1.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations:\n",
    "- Lack of a GPU enabled system \n",
    "    training taking a longer time (1000 iterations took 3 hours on a CPU driven iteration) \n",
    "    Lower iterations (say around less than 1000) does not produce an identifiable resultant image.\n",
    "-    Memory dumps while running larger iterations\n",
    "-    TensorFlow compatibility with python version on Jupyter notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "Anish Athalye [Implementation of Neural Style in TensorFlow](https://github.com/anishathalye/neural-style)\n",
    "\n",
    "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge [A Neural Algorithm of Artistic Style](https://github.com/leongatys/fast-neural-style)\n",
    "\n",
    "Justin Johnson, Alexandre Alahi, Li Fei-Fei [Perceptual losses for real-time style transfer and super-resolution](https://github.com/jcjohnson/fast-neural-style)\n",
    "\n",
    "Logan Engstrom [Fast Style Transfer](https://github.com/lengstrom/fast-style-transfer/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
